{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "63019e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pymorphy2\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Инициализация морфологического анализатора\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34fe68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Загрузка стоп-слов для русского языка (раскомментировать при первом запуске)\n",
    "# # nltk.download('stopwords')\n",
    "# russian_stop_words = stopwords.words('russian')\n",
    "\n",
    "# # Сохранение стоп-слов в файл\n",
    "# joblib.dump(russian_stop_words, \"/Users/tomilovdima/good_bad_news/data/russian_stop_words\")\n",
    "\n",
    "# # Чтение исходных данных\n",
    "# file_path1 = \"/Users/tomilovdima/good_bad_news/data/dataset.txt\"\n",
    "# data = []\n",
    "\n",
    "# with open(file_path1, 'r', encoding=\"utf-8\") as file:\n",
    "#     for line in file:\n",
    "#         line = line.strip()\n",
    "#         labels_part, text = line.split(\" \", 1)\n",
    "#         labels = labels_part.split(\",\")\n",
    "#         text = text.lower()  # Приведение к нижнему регистру\n",
    "#         data.append({\n",
    "#             \"text\": text,\n",
    "#             \"labels\": labels\n",
    "#         })\n",
    "\n",
    "# # Создание DataFrame\n",
    "# df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Преобразование multi-label в binary классификацию (TOXIC/NORMAL)\n",
    "# df[\"TOXIC\"] = df[\"labels\"].apply(lambda x: 1 if \"__label__INSULT\" in x or \"__label__THREAT\" in x or \"__label__OBSCENITY\" in x else 0)\n",
    "# df[\"NORMAL\"] = df[\"labels\"].apply(lambda x: 1 if \"__label__NORMAL\" in x else 0)\n",
    "# df = df.drop(columns=\"labels\")\n",
    "\n",
    "# # Подготовка признаков и целевой переменной\n",
    "# X = df[\"text\"]\n",
    "# y = df[\"TOXIC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612cb45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Функция предобработки для русского текста (раскомментировать для повторной обработки)\n",
    "# def preprocess(text):\n",
    "#     if not isinstance(text, str):\n",
    "#         return \"\"\n",
    "#     text = text.lower()\n",
    "\n",
    "#     # Удаление символов, кроме букв и пробелов\n",
    "#     text = re.sub(r'[^а-яё\\s]', ' ', text)\n",
    "\n",
    "#     # Удаление лишних пробелов\n",
    "#     text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "#     # Токенизация\n",
    "#     tokens = text.split()\n",
    "\n",
    "#     # Удаление стоп-слов и лемматизация\n",
    "#     lemmas = []\n",
    "#     for token in tokens:\n",
    "#         if token not in russian_stop_words and len(token) > 2:\n",
    "#             parsed = morph.parse(token)[0]\n",
    "#             lemmas.append(parsed.normal_form)\n",
    "    \n",
    "#     return ' '.join(lemmas)\n",
    "\n",
    "# # Применение предобработки (раскомментировать для запуска)\n",
    "# X_processed = [preprocess(text) for text in X]\n",
    "# df_clean = pd.DataFrame({'text': X_processed, 'label': y})\n",
    "# df_clean.to_csv('/Users/tomilovdima/good_bad_news/data/X_processed.pkl', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6f28f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка предобработанных данных\n",
    "df = pd.read_csv('/Users/tomilovdima/good_bad_news/data/X_processed.pkl')\n",
    "X_processed = df['text'].values.astype('U')  # Преобразование к Unicode для совместимости\n",
    "y = df['label'].values\n",
    "X_processed = np.array(X_processed)\n",
    "\n",
    "# Разделение на train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, \n",
    "    test_size=0.2, \n",
    "    random_state=1, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec586c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/tomilovdima/good_bad_news/artefacts/toxic_model_v1.pkl']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Пайплайн для GridSearch (раскомментировать для обучения)\n",
    "# pipeline = Pipeline([\n",
    "#     ('vectorizer', TfidfVectorizer()),\n",
    "#     # Оборачиваем LinearSVC для получения вероятностей\n",
    "#     ('clf', CalibratedClassifierCV(LinearSVC(max_iter=2000, dual='auto')))\n",
    "# ])\n",
    "\n",
    "# # Параметры для RandomizedSearch (раскомментировать для обучения)\n",
    "# param_grid = [\n",
    "#     {\n",
    "#         'vectorizer__max_df': [0.7, 0.9],         # Игнорировать слишком частые слова\n",
    "#         'vectorizer__min_df': [2, 5],             # Игнорировать слишком редкие слова\n",
    "#         'vectorizer__use_idf': [True, False],     # Взвешивание IDF\n",
    "#         'vectorizer__max_features': [10000, 20000, 30000],\n",
    "#         'clf__estimator__C': [0.1, 1, 10, 100],   # Сила регуляризации\n",
    "#         'clf__method': ['sigmoid', 'isotonic']    # Способы калибровки\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# # Создание RandomizedSearch (раскомментировать для обучения)\n",
    "# random_search = RandomizedSearchCV(pipeline, param_distributions=param_grid, n_iter=20, cv=3)\n",
    "# random_search.fit(X_train, y_train)\n",
    "# best_model = random_search.best_estimator_\n",
    "# joblib.dump(best_model, \"/Users/tomilovdima/good_bad_news/artefacts/toxic_model_v1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d9294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вероятность токсичности: 0.02\n"
     ]
    }
   ],
   "source": [
    "# Загрузка сохраненной модели (раскомментировать для использования)\n",
    "model = joblib.load(\"/Users/tomilovdima/good_bad_news/artefacts/toxic_model_v1.pkl\")\n",
    "\n",
    "# Пример предсказания\n",
    "# new_text = [\"Ты очень плохой человек\"]\n",
    "# probs = model.predict_proba(new_text)[:, 1]\n",
    "# print(f\"Вероятность токсичности: {probs[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e6693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     40714\n",
      "           1       0.93      0.89      0.91      8944\n",
      "\n",
      "    accuracy                           0.97     49658\n",
      "   macro avg       0.96      0.94      0.95     49658\n",
      "weighted avg       0.97      0.97      0.97     49658\n",
      "\n",
      "Confusion Matrix:\n",
      " [[40156   558]\n",
      " [  977  7967]]\n"
     ]
    }
   ],
   "source": [
    "# Предсказание на тестовой выборке и оценка \n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Комментарий по метрикам:\n",
    "# precision для NORMAL = 0.98 - из всех сообщений, которые модель пометила как нормальные, 98% действительно нормальные.\n",
    "# recall для NORMAL = 0.99 - из всех реально нормальных сообщений модель правильно определила 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b91705f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
