
# Детектор токсичных комментариев

Модель машинного обучения для автоматического определения токсичности в русскоязычных текстовых комментариях. Проект включает классификатор на основе SVM/TF-IDF, REST API на FastAPI и готовую Docker-инфраструктуру для быстрого развертывания.

## Быстрый старт

Запустите проект одной командой:

```bash
docker-compose up
```

После запуска:
*   **API сервер** будет доступен по адресу: `http://localhost:8000`
*   **Интерактивная документация (Swagger UI)**: `http://localhost:8000/docs`
*   **Веб-интерфейс для тестирования**: `http://localhost:8000`

## Веб-интерфейс

![Веб-интерфейс](web-interface.png)


## Возможности

*   **Классификация текста**: Определение токсичности (TOXIC/NORMAL) с вероятностной оценкой
*   **Поддержка русского языка**: Специализированная предобработка с лемматизацией
*   **REST API**: Документированное API для интеграции
*   **Веб-интерфейс**: Простой UI для ручного тестирования
*   **Docker контейнеризация**: Готовое окружение для развертывания

## Использование

### 1. Через веб-интерфейс
1.  Откройте `http://localhost:8000` в браузере
2.  Введите текст для проверки в текстовое поле
3.  Нажмите кнопку "Анализировать текст"
4.  Получите результат с меткой и вероятностью

### 2. Через API

**POST запрос к `/predict`:**
```bash
curl -X 'POST' \
  'http://localhost:8000/predict' \
  -H 'Content-Type: application/json' \
  -d '{"text": "Ваш текст для анализа"}'
```

**Ответ:**
```json
{
  "toxicity_score": 2
}
```

## Модель и алгоритмы

### Архитектура
- **Векторизатор**: TF-IDF с настраиваемыми параметрами
- **Классификатор**: LinearSVC с калибровкой вероятностей (CalibratedClassifierCV)
- **Ансамбль**: Pipeline с последовательной обработкой

### Предобработка текста
1.  Чистка и обработка текста
2.  Токенизация и удаление стоп-слов
3.  Лемматизация (pymorphy3)

### Метрики производительности
```
              precision    recall  f1-score   support

      NORMAL       0.98      0.99      0.99      [40714]
       TOXIC       0.93      0.89      0.91      [8944]

    accuracy                           0.97      [49658]
   macro avg       0.96      0.94      0.95      [49658]
weighted avg       0.97      0.97      0.97      [49658]
```

*Точность на тестовой выборке: 98%*


### Использование docker-compose для развертывания
```bash
# Запуск с логами
docker-compose up

# Запуск в фоновом режиме
docker-compose up -d

# Остановка
docker-compose down

# Просмотр логов
docker-compose logs -f
```

## Данные

### Формат датасета
Исходный датасет (`data/dataset.txt`) содержит тексты с мульти-лейбл разметкой:
```
__label__NORMAL,__label__INSULT Текст комментария...
__label__OBSCENITY Текст комментария...
```

### Преобразование в бинарную классификацию
Токсичными считаются комментарии с метками:
- `__label__INSULT` (оскорбление)
- `__label__THREAT` (угроза)  
- `__label__OBSCENITY` (нецензурная лексика)

## Переобучение модели

### 1. Подготовка данных
Поместите новый датасет в формате `dataset.txt` в папку `data/`

### 2. Раскомментирование кода
В файле `notebook_with_model_learning.ipynb` раскомментируйте секции:
- Загрузка и предобработка данных
- Определение пайплайна и параметров GridSearch
- Обучение и сохранение модели


Новая модель сохранится в `artefacts/toxic_model_v2.pkl`


## Технологический стек

- **Python 3.9+** - основной язык разработки
- **FastAPI** - веб-фреймворк для API
- **Scikit-learn** - машинное обучение
- **Docker** - контейнеризация
- **Pymorphy3** - морфологический анализ русского языка
- **Jupyter** - исследовательский анализ

## Дорожная карта развития

- [ ] Интеграция нейросетевых моделей (BERT, RuBERT)
- [ ] Поддержка других языков


## Автор

blessrr

